{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales Performance Analysis Workflow\n",
    "\n",
    "Complete analysis workflow using the **Sales Performance Dataset** to demonstrate the Data Analysis Platform capabilities.\n",
    "\n",
    "## Dataset Overview\n",
    "- **500 sales records** across multiple regions and products\n",
    "- **Key metrics**: Revenue, customer satisfaction, regional performance\n",
    "- **Business questions**: Which regions/products perform best? What drives customer satisfaction?\n",
    "\n",
    "## Analysis Goals\n",
    "1. Explore sales patterns by region and product\n",
    "2. Analyze factors affecting revenue\n",
    "3. Build predictive model for customer satisfaction\n",
    "4. Identify actionable business insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"üìä Sales Analysis Environment Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sales performance dataset\n",
    "# Option 1: Load from sample_datasets (if available locally)\n",
    "try:\n",
    "    df = pd.read_csv('../sample_datasets/sales_performance.csv')\n",
    "    print(\"‚úÖ Loaded from local sample_datasets folder\")\n",
    "except FileNotFoundError:\n",
    "    # Option 2: Load from platform API (after upload)\n",
    "    print(\"‚ùå Local file not found. Please upload sales_performance.csv using the file upload interface.\")\n",
    "    print(\"   Then use the generated code to load via API.\")\n",
    "    # Uncomment and modify with your dataset ID:\n",
    "    # import requests\n",
    "    # DATASET_ID = \"your-dataset-id-here\"\n",
    "    # API_BASE_URL = \"http://localhost:8000\"\n",
    "    # AUTH_TOKEN = \"your-auth-token\"\n",
    "    # headers = {'Authorization': f'Bearer {AUTH_TOKEN}'}\n",
    "    # response = requests.get(f'{API_BASE_URL}/data/{DATASET_ID}', headers=headers)\n",
    "    # df = pd.read_csv(response.json()['file_path'])\n",
    "    \n",
    "    # For demo purposes, create minimal dataset\n",
    "    df = pd.DataFrame({\n",
    "        'region': ['North', 'South'] * 10,\n",
    "        'product': ['Product_A', 'Product_B'] * 10,\n",
    "        'revenue': np.random.uniform(1000, 5000, 20),\n",
    "        'customer_satisfaction': np.random.uniform(6, 10, 20)\n",
    "    })\n",
    "    print(\"üìù Using demo dataset for illustration\")\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\nüìä Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset overview\n",
    "print(\"üìã DATASET OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Data types and basic statistics\n",
    "print(\"\\nüìä DATA TYPES AND STATISTICS\")\n",
    "print(df.info())\n",
    "print(\"\\nNumeric columns summary:\")\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "if len(numeric_cols) > 0:\n",
    "    display(df[numeric_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales distribution by key categorical variables\n",
    "categorical_cols = ['region', 'product', 'quarter'] if 'quarter' in df.columns else ['region', 'product']\n",
    "categorical_cols = [col for col in categorical_cols if col in df.columns]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(categorical_cols), figsize=(6*len(categorical_cols), 5))\n",
    "if len(categorical_cols) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    if i < len(axes):\n",
    "        if col in df.columns:\n",
    "            df[col].value_counts().plot(kind='bar', ax=axes[i])\n",
    "            axes[i].set_title(f'Distribution of {col.title()}')\n",
    "            axes[i].set_xlabel(col.title())\n",
    "            axes[i].set_ylabel('Count')\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Revenue analysis\n",
    "if 'revenue' in df.columns:\n",
    "    print(f\"\\nüí∞ REVENUE ANALYSIS\")\n",
    "    print(f\"Total Revenue: ${df['revenue'].sum():,.2f}\")\n",
    "    print(f\"Average Revenue per Sale: ${df['revenue'].mean():.2f}\")\n",
    "    print(f\"Revenue Range: ${df['revenue'].min():.2f} - ${df['revenue'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regional and product performance analysis\n",
    "if 'region' in df.columns and 'revenue' in df.columns:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Revenue by region\n",
    "    region_revenue = df.groupby('region')['revenue'].agg(['sum', 'mean', 'count'])\n",
    "    region_revenue['sum'].plot(kind='bar', ax=axes[0,0])\n",
    "    axes[0,0].set_title('Total Revenue by Region')\n",
    "    axes[0,0].set_ylabel('Total Revenue ($)')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Average revenue by region\n",
    "    region_revenue['mean'].plot(kind='bar', ax=axes[0,1], color='orange')\n",
    "    axes[0,1].set_title('Average Revenue by Region')\n",
    "    axes[0,1].set_ylabel('Average Revenue ($)')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    if 'product' in df.columns:\n",
    "        # Revenue by product\n",
    "        product_revenue = df.groupby('product')['revenue'].agg(['sum', 'mean'])\n",
    "        product_revenue['sum'].plot(kind='bar', ax=axes[1,0], color='green')\n",
    "        axes[1,0].set_title('Total Revenue by Product')\n",
    "        axes[1,0].set_ylabel('Total Revenue ($)')\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Revenue by region and product (heatmap)\n",
    "        pivot_table = df.pivot_table(values='revenue', index='region', columns='product', aggfunc='sum')\n",
    "        sns.heatmap(pivot_table, annot=True, fmt='.0f', ax=axes[1,1], cmap='Blues')\n",
    "        axes[1,1].set_title('Revenue Heatmap: Region vs Product')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance summary\n",
    "    print(\"\\nüèÜ PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Top performing regions:\")\n",
    "    print(region_revenue.sort_values('sum', ascending=False)[['sum', 'mean']])\n",
    "    \n",
    "    if 'product' in df.columns:\n",
    "        print(\"\\nTop performing products:\")\n",
    "        print(product_revenue.sort_values('sum', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Customer Satisfaction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer satisfaction analysis\n",
    "if 'customer_satisfaction' in df.columns:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Satisfaction distribution\n",
    "    df['customer_satisfaction'].hist(bins=20, ax=axes[0,0])\n",
    "    axes[0,0].set_title('Customer Satisfaction Distribution')\n",
    "    axes[0,0].set_xlabel('Satisfaction Score')\n",
    "    axes[0,0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Satisfaction vs Revenue\n",
    "    if 'revenue' in df.columns:\n",
    "        axes[0,1].scatter(df['customer_satisfaction'], df['revenue'], alpha=0.6)\n",
    "        axes[0,1].set_title('Revenue vs Customer Satisfaction')\n",
    "        axes[0,1].set_xlabel('Customer Satisfaction')\n",
    "        axes[0,1].set_ylabel('Revenue ($)')\n",
    "        \n",
    "        # Calculate correlation\n",
    "        corr = df['customer_satisfaction'].corr(df['revenue'])\n",
    "        axes[0,1].text(0.05, 0.95, f'Correlation: {corr:.3f}', \n",
    "                      transform=axes[0,1].transAxes, \n",
    "                      bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Satisfaction by region\n",
    "    if 'region' in df.columns:\n",
    "        df.boxplot(column='customer_satisfaction', by='region', ax=axes[1,0])\n",
    "        axes[1,0].set_title('Customer Satisfaction by Region')\n",
    "        axes[1,0].set_xlabel('Region')\n",
    "        axes[1,0].set_ylabel('Satisfaction Score')\n",
    "    \n",
    "    # Satisfaction by product\n",
    "    if 'product' in df.columns:\n",
    "        df.boxplot(column='customer_satisfaction', by='product', ax=axes[1,1])\n",
    "        axes[1,1].set_title('Customer Satisfaction by Product')\n",
    "        axes[1,1].set_xlabel('Product')\n",
    "        axes[1,1].set_ylabel('Satisfaction Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Satisfaction statistics\n",
    "    print(\"\\nüòä CUSTOMER SATISFACTION METRICS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Overall Satisfaction: {df['customer_satisfaction'].mean():.2f} ¬± {df['customer_satisfaction'].std():.2f}\")\n",
    "    print(f\"Satisfaction Range: {df['customer_satisfaction'].min():.1f} - {df['customer_satisfaction'].max():.1f}\")\n",
    "    \n",
    "    if 'region' in df.columns:\n",
    "        print(\"\\nSatisfaction by Region:\")\n",
    "        region_satisfaction = df.groupby('region')['customer_satisfaction'].agg(['mean', 'std', 'count'])\n",
    "        print(region_satisfaction.sort_values('mean', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis for numeric variables\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "if len(numeric_cols) > 1:\n",
    "    # Correlation matrix\n",
    "    correlation_matrix = df[numeric_cols].corr()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,\n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Correlation Matrix of Numeric Variables')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Strong correlations\n",
    "    print(\"\\nüîó CORRELATION INSIGHTS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    strong_correlations = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_val = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.3:  # Moderate to strong correlation\n",
    "                strong_correlations.append({\n",
    "                    'Variable_1': correlation_matrix.columns[i],\n",
    "                    'Variable_2': correlation_matrix.columns[j],\n",
    "                    'Correlation': corr_val\n",
    "                })\n",
    "    \n",
    "    if strong_correlations:\n",
    "        corr_df = pd.DataFrame(strong_correlations)\n",
    "        corr_df = corr_df.sort_values('Correlation', key=abs, ascending=False)\n",
    "        print(\"Moderate to strong correlations (|r| > 0.3):\")\n",
    "        for _, row in corr_df.iterrows():\n",
    "            direction = \"positively\" if row['Correlation'] > 0 else \"negatively\"\n",
    "            print(f\"  ‚Ä¢ {row['Variable_1']} and {row['Variable_2']} are {direction} correlated (r={row['Correlation']:.3f})\")\n",
    "    else:\n",
    "        print(\"No moderate to strong correlations found.\")\n",
    "else:\n",
    "    print(\"Not enough numeric variables for correlation analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predictive Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "# We'll predict customer satisfaction based on available features\n",
    "\n",
    "if 'customer_satisfaction' in df.columns:\n",
    "    print(\"ü§ñ BUILDING PREDICTIVE MODEL FOR CUSTOMER SATISFACTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Select features and target\n",
    "    target = 'customer_satisfaction'\n",
    "    feature_candidates = [col for col in df.columns if col != target]\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    df_encoded = df.copy()\n",
    "    label_encoders = {}\n",
    "    \n",
    "    for col in feature_candidates:\n",
    "        if df[col].dtype == 'object':\n",
    "            le = LabelEncoder()\n",
    "            df_encoded[col] = le.fit_transform(df[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    # Select numeric features for modeling\n",
    "    X = df_encoded[feature_candidates].select_dtypes(include=[np.number])\n",
    "    y = df_encoded[target]\n",
    "    \n",
    "    print(f\"Features used: {list(X.columns)}\")\n",
    "    print(f\"Target variable: {target}\")\n",
    "    print(f\"Dataset size: {len(X)} samples, {len(X.columns)} features\")\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(f\"Training set: {len(X_train)} samples\")\n",
    "    print(f\"Test set: {len(X_test)} samples\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Customer satisfaction column not found. Skipping modeling.\")\n",
    "    X_train = X_test = y_train = y_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "if X_train is not None:\n",
    "    # Train multiple models\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    }\n",
    "    \n",
    "    model_results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_r2 = r2_score(y_train, y_pred_train)\n",
    "        test_r2 = r2_score(y_test, y_pred_test)\n",
    "        test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "        \n",
    "        model_results[name] = {\n",
    "            'model': model,\n",
    "            'train_r2': train_r2,\n",
    "            'test_r2': test_r2,\n",
    "            'test_mse': test_mse,\n",
    "            'test_mae': test_mae,\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'predictions': y_pred_test\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{name} Results:\")\n",
    "        print(f\"  Train R¬≤: {train_r2:.4f}\")\n",
    "        print(f\"  Test R¬≤: {test_r2:.4f}\")\n",
    "        print(f\"  Test MSE: {test_mse:.4f}\")\n",
    "        print(f\"  Test MAE: {test_mae:.4f}\")\n",
    "        print(f\"  CV R¬≤ (mean ¬± std): {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "    \n",
    "    # Select best model\n",
    "    best_model_name = max(model_results.keys(), key=lambda k: model_results[k]['test_r2'])\n",
    "    best_model = model_results[best_model_name]['model']\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "    print(f\"Test R¬≤: {model_results[best_model_name]['test_r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation visualizations\n",
    "if X_train is not None and len(model_results) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Predicted vs Actual for best model\n",
    "    best_predictions = model_results[best_model_name]['predictions']\n",
    "    axes[0,0].scatter(y_test, best_predictions, alpha=0.6)\n",
    "    axes[0,0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    axes[0,0].set_xlabel('Actual Satisfaction')\n",
    "    axes[0,0].set_ylabel('Predicted Satisfaction')\n",
    "    axes[0,0].set_title(f'Predicted vs Actual ({best_model_name})')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residuals plot\n",
    "    residuals = y_test - best_predictions\n",
    "    axes[0,1].scatter(best_predictions, residuals, alpha=0.6)\n",
    "    axes[0,1].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[0,1].set_xlabel('Predicted Satisfaction')\n",
    "    axes[0,1].set_ylabel('Residuals')\n",
    "    axes[0,1].set_title('Residuals Plot')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Model comparison\n",
    "    model_names = list(model_results.keys())\n",
    "    test_r2_scores = [model_results[name]['test_r2'] for name in model_names]\n",
    "    cv_means = [model_results[name]['cv_mean'] for name in model_names]\n",
    "    \n",
    "    x_pos = np.arange(len(model_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[1,0].bar(x_pos - width/2, test_r2_scores, width, label='Test R¬≤', alpha=0.8)\n",
    "    axes[1,0].bar(x_pos + width/2, cv_means, width, label='CV R¬≤ Mean', alpha=0.8)\n",
    "    axes[1,0].set_xlabel('Models')\n",
    "    axes[1,0].set_ylabel('R¬≤ Score')\n",
    "    axes[1,0].set_title('Model Performance Comparison')\n",
    "    axes[1,0].set_xticks(x_pos)\n",
    "    axes[1,0].set_xticklabels(model_names)\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Feature importance (for Random Forest)\n",
    "    if best_model_name == 'Random Forest' and hasattr(best_model, 'feature_importances_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': best_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=True)\n",
    "        \n",
    "        axes[1,1].barh(feature_importance['feature'], feature_importance['importance'])\n",
    "        axes[1,1].set_xlabel('Feature Importance')\n",
    "        axes[1,1].set_title('Feature Importance (Random Forest)')\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1,1].text(0.5, 0.5, 'Feature importance\\nonly available for\\nRandom Forest', \n",
    "                      ha='center', va='center', transform=axes[1,1].transAxes)\n",
    "        axes[1,1].set_title('Feature Importance')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Business Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate business insights\n",
    "print(\"üéØ BUSINESS INSIGHTS AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "insights = []\n",
    "\n",
    "# Regional performance insights\n",
    "if 'region' in df.columns and 'revenue' in df.columns:\n",
    "    region_performance = df.groupby('region').agg({\n",
    "        'revenue': ['sum', 'mean', 'count'],\n",
    "        'customer_satisfaction': 'mean' if 'customer_satisfaction' in df.columns else lambda x: None\n",
    "    }).round(2)\n",
    "    \n",
    "    # Flatten column names\n",
    "    region_performance.columns = ['_'.join(col).strip() for col in region_performance.columns]\n",
    "    \n",
    "    top_revenue_region = region_performance['revenue_sum'].idxmax()\n",
    "    top_avg_region = region_performance['revenue_mean'].idxmax()\n",
    "    \n",
    "    insights.append(f\"üí∞ **Revenue Leaders**: {top_revenue_region} generates highest total revenue, while {top_avg_region} has highest average per sale\")\n",
    "    \n",
    "    if 'customer_satisfaction' in df.columns:\n",
    "        region_performance_clean = region_performance.dropna()\n",
    "        if not region_performance_clean.empty:\n",
    "            satisfaction_col = [col for col in region_performance_clean.columns if 'satisfaction' in col][0]\n",
    "            top_satisfaction_region = region_performance_clean[satisfaction_col].idxmax()\n",
    "            insights.append(f\"üòä **Customer Satisfaction**: {top_satisfaction_region} region has the highest customer satisfaction\")\n",
    "\n",
    "# Product performance insights\n",
    "if 'product' in df.columns and 'revenue' in df.columns:\n",
    "    product_performance = df.groupby('product')['revenue'].sum().sort_values(ascending=False)\n",
    "    top_product = product_performance.index[0]\n",
    "    insights.append(f\"üì¶ **Product Performance**: {top_product} is the top revenue generator\")\n",
    "\n",
    "# Model insights\n",
    "if X_train is not None and len(model_results) > 0:\n",
    "    best_r2 = model_results[best_model_name]['test_r2']\n",
    "    if best_r2 > 0.7:\n",
    "        insights.append(f\"ü§ñ **Predictive Accuracy**: Customer satisfaction is highly predictable (R¬≤ = {best_r2:.3f}) using our model\")\n",
    "    elif best_r2 > 0.5:\n",
    "        insights.append(f\"ü§ñ **Predictive Accuracy**: Customer satisfaction is moderately predictable (R¬≤ = {best_r2:.3f})\")\n",
    "    else:\n",
    "        insights.append(f\"ü§ñ **Predictive Challenge**: Customer satisfaction is difficult to predict (R¬≤ = {best_r2:.3f}) - may need more features\")\n",
    "\n",
    "# Correlation insights\n",
    "if 'customer_satisfaction' in df.columns and 'revenue' in df.columns:\n",
    "    satisfaction_revenue_corr = df['customer_satisfaction'].corr(df['revenue'])\n",
    "    if satisfaction_revenue_corr > 0.3:\n",
    "        insights.append(f\"üìà **Revenue-Satisfaction Link**: Strong positive correlation ({satisfaction_revenue_corr:.3f}) between satisfaction and revenue\")\n",
    "    elif satisfaction_revenue_corr < -0.3:\n",
    "        insights.append(f\"üìâ **Revenue-Satisfaction Concern**: Negative correlation ({satisfaction_revenue_corr:.3f}) - higher revenue associated with lower satisfaction\")\n",
    "\n",
    "# Display insights\n",
    "for i, insight in enumerate(insights, 1):\n",
    "    print(f\"{i}. {insight}\")\n",
    "\n",
    "print(\"\\nüìã ACTION RECOMMENDATIONS:\")\n",
    "print(\"1. Focus marketing efforts on high-performing regions and products\")\n",
    "print(\"2. Investigate factors behind regional satisfaction differences\")\n",
    "print(\"3. Use predictive model to identify at-risk customers\")\n",
    "print(\"4. Implement satisfaction monitoring for revenue optimization\")\n",
    "print(\"5. Consider A/B testing in underperforming segments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results summary\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "results_summary = {\n",
    "    'analysis_type': 'Sales Performance Analysis',\n",
    "    'dataset_info': {\n",
    "        'name': 'sales_performance.csv',\n",
    "        'shape': df.shape,\n",
    "        'columns': list(df.columns)\n",
    "    },\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'key_findings': insights,\n",
    "    'performance_metrics': {}\n",
    "}\n",
    "\n",
    "# Add regional performance\n",
    "if 'region' in df.columns and 'revenue' in df.columns:\n",
    "    regional_summary = df.groupby('region').agg({\n",
    "        'revenue': ['sum', 'mean'],\n",
    "        'customer_satisfaction': 'mean' if 'customer_satisfaction' in df.columns else lambda x: None\n",
    "    })\n",
    "    results_summary['regional_performance'] = regional_summary.to_dict()\n",
    "\n",
    "# Add model performance\n",
    "if X_train is not None and len(model_results) > 0:\n",
    "    results_summary['model_performance'] = {\n",
    "        'best_model': best_model_name,\n",
    "        'test_r2': float(model_results[best_model_name]['test_r2']),\n",
    "        'test_mse': float(model_results[best_model_name]['test_mse']),\n",
    "        'features_used': list(X.columns)\n",
    "    }\n",
    "\n",
    "# Save results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_filename = f\"sales_analysis_results_{timestamp}.json\"\n",
    "\n",
    "# Create exports directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('../data/processed/exports', exist_ok=True)\n",
    "\n",
    "with open(f'../data/processed/exports/{results_filename}', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"‚úÖ Analysis results saved to: ../data/processed/exports/{results_filename}\")\n",
    "\n",
    "# Save prediction results if available\n",
    "if X_train is not None and len(model_results) > 0:\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'actual_satisfaction': y_test,\n",
    "        'predicted_satisfaction': best_predictions,\n",
    "        'residual': y_test - best_predictions\n",
    "    })\n",
    "    \n",
    "    predictions_filename = f\"sales_predictions_{timestamp}.csv\"\n",
    "    predictions_df.to_csv(f'../data/processed/exports/{predictions_filename}', index=False)\n",
    "    print(f\"‚úÖ Predictions saved to: ../data/processed/exports/{predictions_filename}\")\n",
    "\n",
    "print(\"\\nüìä Analysis Summary:\")\n",
    "print(f\"  ‚Ä¢ Dataset: {df.shape[0]} sales records analyzed\")\n",
    "print(f\"  ‚Ä¢ Business insights: {len(insights)} key findings\")\n",
    "if X_train is not None:\n",
    "    print(f\"  ‚Ä¢ Model accuracy: {model_results[best_model_name]['test_r2']:.1%} (R¬≤)\")\nprint(f\"  ‚Ä¢ Export files: 2 files saved to exports directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Analysis Completed ‚úÖ\n",
    "\n",
    "This workflow demonstrated a complete sales performance analysis using the Data Analysis Platform:\n",
    "\n",
    "**üìä Exploratory Analysis**\n",
    "- Regional and product performance comparison\n",
    "- Revenue distribution and trends\n",
    "- Customer satisfaction patterns\n",
    "\n",
    "**üîç Statistical Analysis**\n",
    "- Correlation analysis between key variables\n",
    "- Performance metrics by region and product\n",
    "- Data quality assessment\n",
    "\n",
    "**ü§ñ Predictive Modeling**\n",
    "- Customer satisfaction prediction\n",
    "- Model comparison (Linear Regression vs Random Forest)\n",
    "- Feature importance analysis\n",
    "\n",
    "**üí° Business Insights**\n",
    "- Actionable recommendations for sales optimization\n",
    "- Performance benchmarks and targets\n",
    "- Strategic focus areas identified\n",
    "\n",
    "### Key Platform Features Used\n",
    "\n",
    "‚úÖ Data loading and validation  \n",
    "‚úÖ Comprehensive EDA capabilities  \n",
    "‚úÖ Statistical analysis tools  \n",
    "‚úÖ Machine learning integration  \n",
    "‚úÖ Visualization suite  \n",
    "‚úÖ Results export functionality  \n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Scale Analysis**: Apply similar workflow to larger datasets\n",
    "2. **Advanced Models**: Try ensemble methods or deep learning\n",
    "3. **Real-time Integration**: Connect to live sales data\n",
    "4. **Dashboard Creation**: Build interactive business dashboards\n",
    "\n",
    "---\n",
    "*Analysis completed using the Data Analysis and Prediction Platform*"
   ]
  }
 ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}