{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales Performance Analysis Workflow\n",
    "\n",
    "Complete analysis workflow using the **Sales Performance Dataset** to demonstrate the Data Analysis Platform capabilities.\n",
    "\n",
    "## Dataset Overview\n",
    "- **500 sales records** across multiple regions and products\n",
    "- **Key metrics**: Revenue, customer satisfaction, regional performance\n",
    "- **Business questions**: Which regions/products perform best? What drives customer satisfaction?\n",
    "\n",
    "## Analysis Goals\n",
    "1. Explore sales patterns by region and product\n",
    "2. Analyze factors affecting revenue\n",
    "3. Build predictive model for customer satisfaction\n",
    "4. Identify actionable business insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"📊 Sales Analysis Environment Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sales performance dataset\n",
    "# Option 1: Load from sample_datasets (if available locally)\n",
    "try:\n",
    "    df = pd.read_csv('../sample_datasets/sales_performance.csv')\n",
    "    print(\"✅ Loaded from local sample_datasets folder\")\n",
    "except FileNotFoundError:\n",
    "    # Option 2: Load from platform API (after upload)\n",
    "    print(\"❌ Local file not found. Please upload sales_performance.csv using the file upload interface.\")\n",
    "    print(\"   Then use the generated code to load via API.\")\n",
    "    # Uncomment and modify with your dataset ID:\n",
    "    # import requests\n",
    "    # DATASET_ID = \"your-dataset-id-here\"\n",
    "    # API_BASE_URL = \"http://localhost:8000\"\n",
    "    # AUTH_TOKEN = \"your-auth-token\"\n",
    "    # headers = {'Authorization': f'Bearer {AUTH_TOKEN}'}\n",
    "    # response = requests.get(f'{API_BASE_URL}/data/{DATASET_ID}', headers=headers)\n",
    "    # df = pd.read_csv(response.json()['file_path'])\n",
    "    \n",
    "    # For demo purposes, create minimal dataset\n",
    "    df = pd.DataFrame({\n",
    "        'region': ['North', 'South'] * 10,\n",
    "        'product': ['Product_A', 'Product_B'] * 10,\n",
    "        'revenue': np.random.uniform(1000, 5000, 20),\n",
    "        'customer_satisfaction': np.random.uniform(6, 10, 20)\n",
    "    })\n",
    "    print(\"📝 Using demo dataset for illustration\")\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\n📊 Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset overview\n",
    "print(\"📋 DATASET OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Data types and basic statistics\n",
    "print(\"\\n📊 DATA TYPES AND STATISTICS\")\n",
    "print(df.info())\n",
    "print(\"\\nNumeric columns summary:\")\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "if len(numeric_cols) > 0:\n",
    "    display(df[numeric_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales distribution by key categorical variables\n",
    "categorical_cols = ['region', 'product', 'quarter'] if 'quarter' in df.columns else ['region', 'product']\n",
    "categorical_cols = [col for col in categorical_cols if col in df.columns]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(categorical_cols), figsize=(6*len(categorical_cols), 5))\n",
    "if len(categorical_cols) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    if i < len(axes):\n",
    "        if col in df.columns:\n",
    "            df[col].value_counts().plot(kind='bar', ax=axes[i])\n",
    "            axes[i].set_title(f'Distribution of {col.title()}')\n",
    "            axes[i].set_xlabel(col.title())\n",
    "            axes[i].set_ylabel('Count')\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Revenue analysis\n",
    "if 'revenue' in df.columns:\n",
    "    print(f\"\\n💰 REVENUE ANALYSIS\")\n",
    "    print(f\"Total Revenue: ${df['revenue'].sum():,.2f}\")\n",
    "    print(f\"Average Revenue per Sale: ${df['revenue'].mean():.2f}\")\n",
    "    print(f\"Revenue Range: ${df['revenue'].min():.2f} - ${df['revenue'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regional and product performance analysis\n",
    "if 'region' in df.columns and 'revenue' in df.columns:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Revenue by region\n",
    "    region_revenue = df.groupby('region')['revenue'].agg(['sum', 'mean', 'count'])\n",
    "    region_revenue['sum'].plot(kind='bar', ax=axes[0,0])\n",
    "    axes[0,0].set_title('Total Revenue by Region')\n",
    "    axes[0,0].set_ylabel('Total Revenue ($)')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Average revenue by region\n",
    "    region_revenue['mean'].plot(kind='bar', ax=axes[0,1], color='orange')\n",
    "    axes[0,1].set_title('Average Revenue by Region')\n",
    "    axes[0,1].set_ylabel('Average Revenue ($)')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    if 'product' in df.columns:\n",
    "        # Revenue by product\n",
    "        product_revenue = df.groupby('product')['revenue'].agg(['sum', 'mean'])\n",
    "        product_revenue['sum'].plot(kind='bar', ax=axes[1,0], color='green')\n",
    "        axes[1,0].set_title('Total Revenue by Product')\n",
    "        axes[1,0].set_ylabel('Total Revenue ($)')\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Revenue by region and product (heatmap)\n",
    "        pivot_table = df.pivot_table(values='revenue', index='region', columns='product', aggfunc='sum')\n",
    "        sns.heatmap(pivot_table, annot=True, fmt='.0f', ax=axes[1,1], cmap='Blues')\n",
    "        axes[1,1].set_title('Revenue Heatmap: Region vs Product')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance summary\n",
    "    print(\"\\n🏆 PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Top performing regions:\")\n",
    "    print(region_revenue.sort_values('sum', ascending=False)[['sum', 'mean']])\n",
    "    \n",
    "    if 'product' in df.columns:\n",
    "        print(\"\\nTop performing products:\")\n",
    "        print(product_revenue.sort_values('sum', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Customer Satisfaction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer satisfaction analysis\n",
    "if 'customer_satisfaction' in df.columns:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Satisfaction distribution\n",
    "    df['customer_satisfaction'].hist(bins=20, ax=axes[0,0])\n",
    "    axes[0,0].set_title('Customer Satisfaction Distribution')\n",
    "    axes[0,0].set_xlabel('Satisfaction Score')\n",
    "    axes[0,0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Satisfaction vs Revenue\n",
    "    if 'revenue' in df.columns:\n",
    "        axes[0,1].scatter(df['customer_satisfaction'], df['revenue'], alpha=0.6)\n",
    "        axes[0,1].set_title('Revenue vs Customer Satisfaction')\n",
    "        axes[0,1].set_xlabel('Customer Satisfaction')\n",
    "        axes[0,1].set_ylabel('Revenue ($)')\n",
    "        \n",
    "        # Calculate correlation\n",
    "        corr = df['customer_satisfaction'].corr(df['revenue'])\n",
    "        axes[0,1].text(0.05, 0.95, f'Correlation: {corr:.3f}', \n",
    "                      transform=axes[0,1].transAxes, \n",
    "                      bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Satisfaction by region\n",
    "    if 'region' in df.columns:\n",
    "        df.boxplot(column='customer_satisfaction', by='region', ax=axes[1,0])\n",
    "        axes[1,0].set_title('Customer Satisfaction by Region')\n",
    "        axes[1,0].set_xlabel('Region')\n",
    "        axes[1,0].set_ylabel('Satisfaction Score')\n",
    "    \n",
    "    # Satisfaction by product\n",
    "    if 'product' in df.columns:\n",
    "        df.boxplot(column='customer_satisfaction', by='product', ax=axes[1,1])\n",
    "        axes[1,1].set_title('Customer Satisfaction by Product')\n",
    "        axes[1,1].set_xlabel('Product')\n",
    "        axes[1,1].set_ylabel('Satisfaction Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Satisfaction statistics\n",
    "    print(\"\\n😊 CUSTOMER SATISFACTION METRICS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Overall Satisfaction: {df['customer_satisfaction'].mean():.2f} ± {df['customer_satisfaction'].std():.2f}\")\n",
    "    print(f\"Satisfaction Range: {df['customer_satisfaction'].min():.1f} - {df['customer_satisfaction'].max():.1f}\")\n",
    "    \n",
    "    if 'region' in df.columns:\n",
    "        print(\"\\nSatisfaction by Region:\")\n",
    "        region_satisfaction = df.groupby('region')['customer_satisfaction'].agg(['mean', 'std', 'count'])\n",
    "        print(region_satisfaction.sort_values('mean', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis for numeric variables\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "if len(numeric_cols) > 1:\n",
    "    # Correlation matrix\n",
    "    correlation_matrix = df[numeric_cols].corr()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,\n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Correlation Matrix of Numeric Variables')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Strong correlations\n",
    "    print(\"\\n🔗 CORRELATION INSIGHTS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    strong_correlations = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_val = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.3:  # Moderate to strong correlation\n",
    "                strong_correlations.append({\n",
    "                    'Variable_1': correlation_matrix.columns[i],\n",
    "                    'Variable_2': correlation_matrix.columns[j],\n",
    "                    'Correlation': corr_val\n",
    "                })\n",
    "    \n",
    "    if strong_correlations:\n",
    "        corr_df = pd.DataFrame(strong_correlations)\n",
    "        corr_df = corr_df.sort_values('Correlation', key=abs, ascending=False)\n",
    "        print(\"Moderate to strong correlations (|r| > 0.3):\")\n",
    "        for _, row in corr_df.iterrows():\n",
    "            direction = \"positively\" if row['Correlation'] > 0 else \"negatively\"\n",
    "            print(f\"  • {row['Variable_1']} and {row['Variable_2']} are {direction} correlated (r={row['Correlation']:.3f})\")\n",
    "    else:\n",
    "        print(\"No moderate to strong correlations found.\")\n",
    "else:\n",
    "    print(\"Not enough numeric variables for correlation analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predictive Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "# We'll predict customer satisfaction based on available features\n",
    "\n",
    "if 'customer_satisfaction' in df.columns:\n",
    "    print(\"🤖 BUILDING PREDICTIVE MODEL FOR CUSTOMER SATISFACTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Select features and target\n",
    "    target = 'customer_satisfaction'\n",
    "    feature_candidates = [col for col in df.columns if col != target]\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    df_encoded = df.copy()\n",
    "    label_encoders = {}\n",
    "    \n",
    "    for col in feature_candidates:\n",
    "        if df[col].dtype == 'object':\n",
    "            le = LabelEncoder()\n",
    "            df_encoded[col] = le.fit_transform(df[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    # Select numeric features for modeling\n",
    "    X = df_encoded[feature_candidates].select_dtypes(include=[np.number])\n",
    "    y = df_encoded[target]\n",
    "    \n",
    "    print(f\"Features used: {list(X.columns)}\")\n",
    "    print(f\"Target variable: {target}\")\n",
    "    print(f\"Dataset size: {len(X)} samples, {len(X.columns)} features\")\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(f\"Training set: {len(X_train)} samples\")\n",
    "    print(f\"Test set: {len(X_test)} samples\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Customer satisfaction column not found. Skipping modeling.\")\n",
    "    X_train = X_test = y_train = y_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "if X_train is not None:\n",
    "    # Train multiple models\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    }\n",
    "    \n",
    "    model_results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_r2 = r2_score(y_train, y_pred_train)\n",
    "        test_r2 = r2_score(y_test, y_pred_test)\n",
    "        test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "        \n",
    "        model_results[name] = {\n",
    "            'model': model,\n",
    "            'train_r2': train_r2,\n",
    "            'test_r2': test_r2,\n",
    "            'test_mse': test_mse,\n",
    "            'test_mae': test_mae,\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'predictions': y_pred_test\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{name} Results:\")\n",
    "        print(f\"  Train R²: {train_r2:.4f}\")\n",
    "        print(f\"  Test R²: {test_r2:.4f}\")\n",
    "        print(f\"  Test MSE: {test_mse:.4f}\")\n",
    "        print(f\"  Test MAE: {test_mae:.4f}\")\n",
    "        print(f\"  CV R² (mean ± std): {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "    \n",
    "    # Select best model\n",
    "    best_model_name = max(model_results.keys(), key=lambda k: model_results[k]['test_r2'])\n",
    "    best_model = model_results[best_model_name]['model']\n",
    "    \n",
    "    print(f\"\\n🏆 Best Model: {best_model_name}\")\n",
    "    print(f\"Test R²: {model_results[best_model_name]['test_r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation visualizations\n",
    "if X_train is not None and len(model_results) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Predicted vs Actual for best model\n",
    "    best_predictions = model_results[best_model_name]['predictions']\n",
    "    axes[0,0].scatter(y_test, best_predictions, alpha=0.6)\n",
    "    axes[0,0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    axes[0,0].set_xlabel('Actual Satisfaction')\n",
    "    axes[0,0].set_ylabel('Predicted Satisfaction')\n",
    "    axes[0,0].set_title(f'Predicted vs Actual ({best_model_name})')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residuals plot\n",
    "    residuals = y_test - best_predictions\n",
    "    axes[0,1].scatter(best_predictions, residuals, alpha=0.6)\n",
    "    axes[0,1].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[0,1].set_xlabel('Predicted Satisfaction')\n",
    "    axes[0,1].set_ylabel('Residuals')\n",
    "    axes[0,1].set_title('Residuals Plot')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Model comparison\n",
    "    model_names = list(model_results.keys())\n",
    "    test_r2_scores = [model_results[name]['test_r2'] for name in model_names]\n",
    "    cv_means = [model_results[name]['cv_mean'] for name in model_names]\n",
    "    \n",
    "    x_pos = np.arange(len(model_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[1,0].bar(x_pos - width/2, test_r2_scores, width, label='Test R²', alpha=0.8)\n",
    "    axes[1,0].bar(x_pos + width/2, cv_means, width, label='CV R² Mean', alpha=0.8)\n",
    "    axes[1,0].set_xlabel('Models')\n",
    "    axes[1,0].set_ylabel('R² Score')\n",
    "    axes[1,0].set_title('Model Performance Comparison')\n",
    "    axes[1,0].set_xticks(x_pos)\n",
    "    axes[1,0].set_xticklabels(model_names)\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Feature importance (for Random Forest)\n",
    "    if best_model_name == 'Random Forest' and hasattr(best_model, 'feature_importances_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': best_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=True)\n",
    "        \n",
    "        axes[1,1].barh(feature_importance['feature'], feature_importance['importance'])\n",
    "        axes[1,1].set_xlabel('Feature Importance')\n",
    "        axes[1,1].set_title('Feature Importance (Random Forest)')\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1,1].text(0.5, 0.5, 'Feature importance\\nonly available for\\nRandom Forest', \n",
    "                      ha='center', va='center', transform=axes[1,1].transAxes)\n",
    "        axes[1,1].set_title('Feature Importance')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Business Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate business insights\n",
    "print(\"🎯 BUSINESS INSIGHTS AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "insights = []\n",
    "\n",
    "# Regional performance insights\n",
    "if 'region' in df.columns and 'revenue' in df.columns:\n",
    "    region_performance = df.groupby('region').agg({\n",
    "        'revenue': ['sum', 'mean', 'count'],\n",
    "        'customer_satisfaction': 'mean' if 'customer_satisfaction' in df.columns else lambda x: None\n",
    "    }).round(2)\n",
    "    \n",
    "    # Flatten column names\n",
    "    region_performance.columns = ['_'.join(col).strip() for col in region_performance.columns]\n",
    "    \n",
    "    top_revenue_region = region_performance['revenue_sum'].idxmax()\n",
    "    top_avg_region = region_performance['revenue_mean'].idxmax()\n",
    "    \n",
    "    insights.append(f\"💰 **Revenue Leaders**: {top_revenue_region} generates highest total revenue, while {top_avg_region} has highest average per sale\")\n",
    "    \n",
    "    if 'customer_satisfaction' in df.columns:\n",
    "        region_performance_clean = region_performance.dropna()\n",
    "        if not region_performance_clean.empty:\n",
    "            satisfaction_col = [col for col in region_performance_clean.columns if 'satisfaction' in col][0]\n",
    "            top_satisfaction_region = region_performance_clean[satisfaction_col].idxmax()\n",
    "            insights.append(f\"😊 **Customer Satisfaction**: {top_satisfaction_region} region has the highest customer satisfaction\")\n",
    "\n",
    "# Product performance insights\n",
    "if 'product' in df.columns and 'revenue' in df.columns:\n",
    "    product_performance = df.groupby('product')['revenue'].sum().sort_values(ascending=False)\n",
    "    top_product = product_performance.index[0]\n",
    "    insights.append(f\"📦 **Product Performance**: {top_product} is the top revenue generator\")\n",
    "\n",
    "# Model insights\n",
    "if X_train is not None and len(model_results) > 0:\n",
    "    best_r2 = model_results[best_model_name]['test_r2']\n",
    "    if best_r2 > 0.7:\n",
    "        insights.append(f\"🤖 **Predictive Accuracy**: Customer satisfaction is highly predictable (R² = {best_r2:.3f}) using our model\")\n",
    "    elif best_r2 > 0.5:\n",
    "        insights.append(f\"🤖 **Predictive Accuracy**: Customer satisfaction is moderately predictable (R² = {best_r2:.3f})\")\n",
    "    else:\n",
    "        insights.append(f\"🤖 **Predictive Challenge**: Customer satisfaction is difficult to predict (R² = {best_r2:.3f}) - may need more features\")\n",
    "\n",
    "# Correlation insights\n",
    "if 'customer_satisfaction' in df.columns and 'revenue' in df.columns:\n",
    "    satisfaction_revenue_corr = df['customer_satisfaction'].corr(df['revenue'])\n",
    "    if satisfaction_revenue_corr > 0.3:\n",
    "        insights.append(f\"📈 **Revenue-Satisfaction Link**: Strong positive correlation ({satisfaction_revenue_corr:.3f}) between satisfaction and revenue\")\n",
    "    elif satisfaction_revenue_corr < -0.3:\n",
    "        insights.append(f\"📉 **Revenue-Satisfaction Concern**: Negative correlation ({satisfaction_revenue_corr:.3f}) - higher revenue associated with lower satisfaction\")\n",
    "\n",
    "# Display insights\n",
    "for i, insight in enumerate(insights, 1):\n",
    "    print(f\"{i}. {insight}\")\n",
    "\n",
    "print(\"\\n📋 ACTION RECOMMENDATIONS:\")\n",
    "print(\"1. Focus marketing efforts on high-performing regions and products\")\n",
    "print(\"2. Investigate factors behind regional satisfaction differences\")\n",
    "print(\"3. Use predictive model to identify at-risk customers\")\n",
    "print(\"4. Implement satisfaction monitoring for revenue optimization\")\n",
    "print(\"5. Consider A/B testing in underperforming segments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results summary\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "results_summary = {\n",
    "    'analysis_type': 'Sales Performance Analysis',\n",
    "    'dataset_info': {\n",
    "        'name': 'sales_performance.csv',\n",
    "        'shape': df.shape,\n",
    "        'columns': list(df.columns)\n",
    "    },\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'key_findings': insights,\n",
    "    'performance_metrics': {}\n",
    "}\n",
    "\n",
    "# Add regional performance\n",
    "if 'region' in df.columns and 'revenue' in df.columns:\n",
    "    regional_summary = df.groupby('region').agg({\n",
    "        'revenue': ['sum', 'mean'],\n",
    "        'customer_satisfaction': 'mean' if 'customer_satisfaction' in df.columns else lambda x: None\n",
    "    })\n",
    "    results_summary['regional_performance'] = regional_summary.to_dict()\n",
    "\n",
    "# Add model performance\n",
    "if X_train is not None and len(model_results) > 0:\n",
    "    results_summary['model_performance'] = {\n",
    "        'best_model': best_model_name,\n",
    "        'test_r2': float(model_results[best_model_name]['test_r2']),\n",
    "        'test_mse': float(model_results[best_model_name]['test_mse']),\n",
    "        'features_used': list(X.columns)\n",
    "    }\n",
    "\n",
    "# Save results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_filename = f\"sales_analysis_results_{timestamp}.json\"\n",
    "\n",
    "# Create exports directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('../data/processed/exports', exist_ok=True)\n",
    "\n",
    "with open(f'../data/processed/exports/{results_filename}', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"✅ Analysis results saved to: ../data/processed/exports/{results_filename}\")\n",
    "\n",
    "# Save prediction results if available\n",
    "if X_train is not None and len(model_results) > 0:\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'actual_satisfaction': y_test,\n",
    "        'predicted_satisfaction': best_predictions,\n",
    "        'residual': y_test - best_predictions\n",
    "    })\n",
    "    \n",
    "    predictions_filename = f\"sales_predictions_{timestamp}.csv\"\n",
    "    predictions_df.to_csv(f'../data/processed/exports/{predictions_filename}', index=False)\n",
    "    print(f\"✅ Predictions saved to: ../data/processed/exports/{predictions_filename}\")\n",
    "\n",
    "print(\"\\n📊 Analysis Summary:\")\n",
    "print(f\"  • Dataset: {df.shape[0]} sales records analyzed\")\n",
    "print(f\"  • Business insights: {len(insights)} key findings\")\n",
    "if X_train is not None:\n",
    "    print(f\"  • Model accuracy: {model_results[best_model_name]['test_r2']:.1%} (R²)\")\nprint(f\"  • Export files: 2 files saved to exports directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Analysis Completed ✅\n",
    "\n",
    "This workflow demonstrated a complete sales performance analysis using the Data Analysis Platform:\n",
    "\n",
    "**📊 Exploratory Analysis**\n",
    "- Regional and product performance comparison\n",
    "- Revenue distribution and trends\n",
    "- Customer satisfaction patterns\n",
    "\n",
    "**🔍 Statistical Analysis**\n",
    "- Correlation analysis between key variables\n",
    "- Performance metrics by region and product\n",
    "- Data quality assessment\n",
    "\n",
    "**🤖 Predictive Modeling**\n",
    "- Customer satisfaction prediction\n",
    "- Model comparison (Linear Regression vs Random Forest)\n",
    "- Feature importance analysis\n",
    "\n",
    "**💡 Business Insights**\n",
    "- Actionable recommendations for sales optimization\n",
    "- Performance benchmarks and targets\n",
    "- Strategic focus areas identified\n",
    "\n",
    "### Key Platform Features Used\n",
    "\n",
    "✅ Data loading and validation  \n",
    "✅ Comprehensive EDA capabilities  \n",
    "✅ Statistical analysis tools  \n",
    "✅ Machine learning integration  \n",
    "✅ Visualization suite  \n",
    "✅ Results export functionality  \n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Scale Analysis**: Apply similar workflow to larger datasets\n",
    "2. **Advanced Models**: Try ensemble methods or deep learning\n",
    "3. **Real-time Integration**: Connect to live sales data\n",
    "4. **Dashboard Creation**: Build interactive business dashboards\n",
    "\n",
    "---\n",
    "*Analysis completed using the Data Analysis and Prediction Platform*"
   ]
  }
 ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}