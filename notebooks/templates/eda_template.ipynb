{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) Template\n",
    "\n",
    "Comprehensive template for exploring and understanding your dataset before modeling.\n",
    "\n",
    "## EDA Objectives\n",
    "- Understand data structure and quality\n",
    "- Identify patterns and relationships\n",
    "- Detect outliers and anomalies\n",
    "- Guide preprocessing decisions\n",
    "- Inform feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"Set2\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"üîç EDA Environment Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "# Replace with your data loading method\n",
    "\n",
    "# Example: Generate sample dataset\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "# Generate mixed data types\n",
    "df = pd.DataFrame({\n",
    "    'numeric_1': np.random.normal(100, 20, n),\n",
    "    'numeric_2': np.random.exponential(2, n),\n",
    "    'numeric_3': np.random.uniform(0, 50, n),\n",
    "    'categorical_1': np.random.choice(['A', 'B', 'C', 'D'], n, p=[0.4, 0.3, 0.2, 0.1]),\n",
    "    'categorical_2': np.random.choice(['X', 'Y', 'Z'], n),\n",
    "    'binary': np.random.choice([0, 1], n, p=[0.7, 0.3]),\n",
    "    'date': pd.date_range('2020-01-01', periods=n, freq='D')[:n]\n",
    "})\n",
    "\n",
    "# Add some missing values\n",
    "missing_idx = np.random.choice(df.index, size=int(0.05 * n), replace=False)\n",
    "df.loc[missing_idx, 'numeric_1'] = np.nan\n",
    "\n",
    "# Add target variable\n",
    "df['target'] = (0.5 * df['numeric_1'].fillna(df['numeric_1'].mean()) + \n",
    "               0.3 * df['numeric_2'] + \n",
    "               (df['categorical_1'] == 'A').astype(int) * 10 +\n",
    "               np.random.normal(0, 5, n))\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"üìä DATASET OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Duplicate Rows: {df.duplicated().sum():,}\")\n",
    "\n",
    "print(\"\\nüìã COLUMN INFORMATION\")\n",
    "print(\"=\" * 50)\n",
    "info_df = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Data_Type': df.dtypes,\n",
    "    'Non_Null_Count': df.count(),\n",
    "    'Null_Count': df.isnull().sum(),\n",
    "    'Null_Percentage': (df.isnull().sum() / len(df) * 100).round(2),\n",
    "    'Unique_Values': [df[col].nunique() for col in df.columns],\n",
    "    'Memory_MB': (df.memory_usage(deep=True)[1:] / 1024**2).round(3)\n",
    "})\n",
    "\n",
    "display(info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types classification\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "\n",
    "print(f\"üî¢ Numeric columns ({len(numeric_cols)}): {numeric_cols}\")\n",
    "print(f\"üè∑Ô∏è  Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"üìÖ Datetime columns ({len(datetime_cols)}): {datetime_cols}\")\n",
    "\n",
    "# Identify potential categorical columns stored as numeric\n",
    "potential_categorical = []\n",
    "for col in numeric_cols:\n",
    "    if df[col].nunique() <= 10 and df[col].dtype in ['int64', 'float64']:\n",
    "        potential_categorical.append(col)\n",
    "\n",
    "if potential_categorical:\n",
    "    print(f\"‚ö†Ô∏è  Potential categorical (stored as numeric): {potential_categorical}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data visualization\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "\n",
    "if missing_data.sum() > 0:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Missing data bar plot\n",
    "    missing_cols = missing_data[missing_data > 0]\n",
    "    ax1.bar(missing_cols.index, missing_cols.values)\n",
    "    ax1.set_title('Missing Data Count by Column')\n",
    "    ax1.set_xlabel('Columns')\n",
    "    ax1.set_ylabel('Missing Count')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Missing data percentage\n",
    "    missing_pct_cols = missing_percent[missing_percent > 0]\n",
    "    ax2.bar(missing_pct_cols.index, missing_pct_cols.values, color='orange')\n",
    "    ax2.set_title('Missing Data Percentage by Column')\n",
    "    ax2.set_xlabel('Columns')\n",
    "    ax2.set_ylabel('Missing Percentage (%)')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Missing data patterns\n",
    "    import missingno as msno\n",
    "    try:\n",
    "        msno.matrix(df, figsize=(12, 6))\n",
    "        plt.title('Missing Data Pattern Matrix')\n",
    "        plt.show()\n",
    "    except:\n",
    "        print(\"Install missingno for advanced missing data visualization: pip install missingno\")\n",
    "else:\n",
    "    print(\"‚úÖ No missing data detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(numeric_cols) > 0:\n",
    "    # Statistical summary\n",
    "    print(\"üìä NUMERIC VARIABLES SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    numeric_summary = df[numeric_cols].describe()\n",
    "    \n",
    "    # Add additional statistics\n",
    "    additional_stats = pd.DataFrame({\n",
    "        'skewness': df[numeric_cols].skew(),\n",
    "        'kurtosis': df[numeric_cols].kurtosis(),\n",
    "        'cv': df[numeric_cols].std() / df[numeric_cols].mean()  # Coefficient of variation\n",
    "    }).T\n",
    "    \n",
    "    full_summary = pd.concat([numeric_summary, additional_stats])\n",
    "    display(full_summary.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots for numeric variables\n",
    "if len(numeric_cols) > 0:\n",
    "    n_cols = min(3, len(numeric_cols))\n",
    "    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes if isinstance(axes, np.ndarray) else [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        if i < len(axes):\n",
    "            # Histogram with KDE\n",
    "            sns.histplot(df[col].dropna(), kde=True, ax=axes[i])\n",
    "            axes[i].set_title(f'Distribution of {col}')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add statistics as text\n",
    "            mean_val = df[col].mean()\n",
    "            median_val = df[col].median()\n",
    "            skew_val = df[col].skew()\n",
    "            \n",
    "            stats_text = f'Mean: {mean_val:.2f}\\nMedian: {median_val:.2f}\\nSkew: {skew_val:.2f}'\n",
    "            axes[i].text(0.05, 0.95, stats_text, transform=axes[i].transAxes, \n",
    "                        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(numeric_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for outlier detection\n",
    "if len(numeric_cols) > 0:\n",
    "    n_cols = min(4, len(numeric_cols))\n",
    "    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 3 * n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes if isinstance(axes, np.ndarray) else [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        if i < len(axes):\n",
    "            sns.boxplot(y=df[col].dropna(), ax=axes[i])\n",
    "            axes[i].set_title(f'Box Plot: {col}')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(numeric_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Outlier detection using IQR method\n",
    "    print(\"\\nüéØ OUTLIER DETECTION (IQR Method)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    outlier_summary = []\n",
    "    for col in numeric_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        outlier_count = len(outliers)\n",
    "        outlier_percentage = (outlier_count / len(df)) * 100\n",
    "        \n",
    "        outlier_summary.append({\n",
    "            'Column': col,\n",
    "            'Outlier_Count': outlier_count,\n",
    "            'Outlier_Percentage': round(outlier_percentage, 2),\n",
    "            'Lower_Bound': round(lower_bound, 3),\n",
    "            'Upper_Bound': round(upper_bound, 3)\n",
    "        })\n",
    "    \n",
    "    outlier_df = pd.DataFrame(outlier_summary)\n",
    "    display(outlier_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(categorical_cols) > 0:\n",
    "    print(\"üè∑Ô∏è  CATEGORICAL VARIABLES SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        print(f\"\\nüìä {col.upper()}:\")\n",
    "        value_counts = df[col].value_counts()\n",
    "        percentages = df[col].value_counts(normalize=True) * 100\n",
    "        \n",
    "        summary_df = pd.DataFrame({\n",
    "            'Count': value_counts,\n",
    "            'Percentage': percentages.round(2)\n",
    "        })\n",
    "        \n",
    "        display(summary_df)\n",
    "        \n",
    "        print(f\"  Unique values: {df[col].nunique()}\")\n",
    "        print(f\"  Most frequent: {df[col].mode()[0]} ({value_counts.iloc[0]} occurrences)\")\n",
    "        \n",
    "        if df[col].nunique() > 20:\n",
    "            print(f\"  ‚ö†Ô∏è High cardinality detected: {df[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize categorical variables\n",
    "if len(categorical_cols) > 0:\n",
    "    n_cols = min(2, len(categorical_cols))\n",
    "    n_rows = (len(categorical_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 4 * n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes if isinstance(axes, np.ndarray) else [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(categorical_cols):\n",
    "        if i < len(axes):\n",
    "            # Create count plot or pie chart based on number of categories\n",
    "            if df[col].nunique() <= 10:\n",
    "                value_counts = df[col].value_counts()\n",
    "                axes[i].pie(value_counts.values, labels=value_counts.index, autopct='%1.1f%%')\n",
    "                axes[i].set_title(f'Distribution of {col}')\n",
    "            else:\n",
    "                # For high cardinality, show top 10\n",
    "                top_values = df[col].value_counts().head(10)\n",
    "                sns.barplot(x=top_values.values, y=top_values.index, ax=axes[i])\n",
    "                axes[i].set_title(f'Top 10 Values in {col}')\n",
    "                axes[i].set_xlabel('Count')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(categorical_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numeric variables\n",
    "if len(numeric_cols) > 1:\n",
    "    correlation_matrix = df[numeric_cols].corr()\n",
    "    \n",
    "    # Create correlation heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,\n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Correlation Matrix of Numeric Variables')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find strong correlations\n",
    "    print(\"\\nüîó STRONG CORRELATIONS (|r| > 0.7)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    strong_corr = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_val = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.7:\n",
    "                strong_corr.append({\n",
    "                    'Variable_1': correlation_matrix.columns[i],\n",
    "                    'Variable_2': correlation_matrix.columns[j],\n",
    "                    'Correlation': round(corr_val, 3)\n",
    "                })\n",
    "    \n",
    "    if strong_corr:\n",
    "        strong_corr_df = pd.DataFrame(strong_corr)\n",
    "        display(strong_corr_df)\n",
    "    else:\n",
    "        print(\"No strong correlations (|r| > 0.7) found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target variable if it exists\n",
    "target_col = 'target'  # Adjust this to your target variable name\n",
    "\n",
    "if target_col in df.columns:\n",
    "    print(f\"üéØ TARGET VARIABLE ANALYSIS: {target_col}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"Data type: {df[target_col].dtype}\")\n",
    "    print(f\"Missing values: {df[target_col].isnull().sum()}\")\n",
    "    \n",
    "    if df[target_col].dtype in ['int64', 'float64']:\n",
    "        # Numeric target\n",
    "        print(f\"\\nNumeric Target Statistics:\")\n",
    "        print(f\"  Mean: {df[target_col].mean():.3f}\")\n",
    "        print(f\"  Median: {df[target_col].median():.3f}\")\n",
    "        print(f\"  Std: {df[target_col].std():.3f}\")\n",
    "        print(f\"  Skewness: {df[target_col].skew():.3f}\")\n",
    "        print(f\"  Range: [{df[target_col].min():.3f}, {df[target_col].max():.3f}]\")\n",
    "        \n",
    "        # Target distribution\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Histogram\n",
    "        sns.histplot(df[target_col].dropna(), kde=True, ax=ax1)\n",
    "        ax1.set_title(f'Distribution of {target_col}')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Box plot\n",
    "        sns.boxplot(y=df[target_col].dropna(), ax=ax2)\n",
    "        ax2.set_title(f'Box Plot of {target_col}')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        # Categorical target\n",
    "        print(f\"\\nCategorical Target:\")\n",
    "        value_counts = df[target_col].value_counts()\n",
    "        print(value_counts)\n",
    "        \n",
    "        # Target distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.countplot(data=df, x=target_col)\n",
    "        plt.title(f'Distribution of {target_col}')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature vs Target Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze relationships between features and target\n",
    "if target_col in df.columns:\n",
    "    print(\"üîç FEATURE-TARGET RELATIONSHIPS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Numeric features vs target\n",
    "    if df[target_col].dtype in ['int64', 'float64']:\n",
    "        # Correlations with target\n",
    "        feature_target_corr = df[numeric_cols].corrwith(df[target_col]).sort_values(key=abs, ascending=False)\n",
    "        \n",
    "        print(\"\\nüìä Correlations with Target:\")\n",
    "        for feature, corr in feature_target_corr.items():\n",
    "            if feature != target_col:\n",
    "                print(f\"  {feature}: {corr:.3f}\")\n",
    "        \n",
    "        # Scatter plots for top correlated features\n",
    "        top_features = feature_target_corr[feature_target_corr.index != target_col].head(4)\n",
    "        \n",
    "        if len(top_features) > 0:\n",
    "            n_cols = min(2, len(top_features))\n",
    "            n_rows = (len(top_features) + n_cols - 1) // n_cols\n",
    "            \n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 4 * n_rows))\n",
    "            if n_rows == 1:\n",
    "                axes = axes if isinstance(axes, np.ndarray) else [axes]\n",
    "            else:\n",
    "                axes = axes.flatten()\n",
    "            \n",
    "            for i, (feature, corr) in enumerate(top_features.items()):\n",
    "                if i < len(axes):\n",
    "                    sns.scatterplot(data=df, x=feature, y=target_col, ax=axes[i], alpha=0.6)\n",
    "                    axes[i].set_title(f'{feature} vs {target_col}\\nCorrelation: {corr:.3f}')\n",
    "                    axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Hide unused subplots\n",
    "            for i in range(len(top_features), len(axes)):\n",
    "                axes[i].set_visible(False)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features vs numeric target\n",
    "if target_col in df.columns and df[target_col].dtype in ['int64', 'float64'] and len(categorical_cols) > 0:\n",
    "    print(\"\\nüìä Categorical Features vs Target:\")\n",
    "    \n",
    "    # Box plots for categorical features\n",
    "    n_cols = min(2, len(categorical_cols))\n",
    "    n_rows = (len(categorical_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(8 * n_cols, 4 * n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes if isinstance(axes, np.ndarray) else [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, cat_col in enumerate(categorical_cols):\n",
    "        if i < len(axes):\n",
    "            sns.boxplot(data=df, x=cat_col, y=target_col, ax=axes[i])\n",
    "            axes[i].set_title(f'{cat_col} vs {target_col}')\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Calculate ANOVA F-statistic\n",
    "            groups = [group[target_col].dropna() for name, group in df.groupby(cat_col)]\n",
    "            if len(groups) > 1:\n",
    "                f_stat, p_value = stats.f_oneway(*groups)\n",
    "                axes[i].text(0.05, 0.95, f'ANOVA p-value: {p_value:.4f}', \n",
    "                           transform=axes[i].transAxes, verticalalignment='top',\n",
    "                           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(categorical_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise relationships (for smaller datasets)\n",
    "if len(numeric_cols) <= 5 and len(df) <= 5000:\n",
    "    print(\"üîó Pairwise Relationships (Pair Plot)\")\n",
    "    \n",
    "    # Select subset of columns for pair plot\n",
    "    pair_cols = numeric_cols[:4]  # Limit to avoid overcrowding\n",
    "    if target_col in df.columns and target_col not in pair_cols:\n",
    "        pair_cols.append(target_col)\n",
    "    \n",
    "    if len(categorical_cols) > 0:\n",
    "        # Use first categorical variable for hue if it has few categories\n",
    "        hue_col = categorical_cols[0] if df[categorical_cols[0]].nunique() <= 5 else None\n",
    "        \n",
    "        sns.pairplot(df[pair_cols + ([hue_col] if hue_col else [])], \n",
    "                    hue=hue_col, diag_kind='hist', plot_kws={'alpha': 0.6})\n",
    "    else:\n",
    "        sns.pairplot(df[pair_cols], diag_kind='hist', plot_kws={'alpha': 0.6})\n",
    "    \n",
    "    plt.suptitle('Pairwise Relationships', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping pair plot due to large dataset or too many variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality summary\n",
    "print(\"\\n‚úÖ DATA QUALITY SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "quality_issues = []\n",
    "\n",
    "# Check for missing data\n",
    "missing_pct = (df.isnull().sum() / len(df) * 100)\n",
    "high_missing = missing_pct[missing_pct > 10]\n",
    "if len(high_missing) > 0:\n",
    "    quality_issues.append(f\"High missing data: {list(high_missing.index)}\")\n",
    "\n",
    "# Check for duplicates\n",
    "if df.duplicated().sum() > 0:\n",
    "    quality_issues.append(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Check for constant columns\n",
    "constant_cols = [col for col in df.columns if df[col].nunique() <= 1]\n",
    "if constant_cols:\n",
    "    quality_issues.append(f\"Constant columns: {constant_cols}\")\n",
    "\n",
    "# Check for high cardinality categorical columns\n",
    "high_cardinality = [col for col in categorical_cols if df[col].nunique() > 50]\n",
    "if high_cardinality:\n",
    "    quality_issues.append(f\"High cardinality categorical: {high_cardinality}\")\n",
    "\n",
    "# Check for skewed distributions\n",
    "highly_skewed = [col for col in numeric_cols if abs(df[col].skew()) > 2]\n",
    "if highly_skewed:\n",
    "    quality_issues.append(f\"Highly skewed distributions: {highly_skewed}\")\n",
    "\n",
    "if quality_issues:\n",
    "    print(\"‚ö†Ô∏è Potential Issues Identified:\")\n",
    "    for issue in quality_issues:\n",
    "        print(f\"  ‚Ä¢ {issue}\")\n",
    "else:\n",
    "    print(\"‚úÖ No major data quality issues detected\")\n",
    "\n",
    "print(\"\\nüìã Recommendations:\")\n",
    "if high_missing.any():\n",
    "    print(f\"  ‚Ä¢ Consider imputation or removal of columns with >10% missing data\")\n",
    "if df.duplicated().sum() > 0:\n",
    "    print(f\"  ‚Ä¢ Remove duplicate rows to avoid data leakage\")\n",
    "if highly_skewed:\n",
    "    print(f\"  ‚Ä¢ Consider log transformation for highly skewed variables\")\n",
    "if high_cardinality:\n",
    "    print(f\"  ‚Ä¢ Consider encoding strategies for high cardinality categorical variables\")\n",
    "\n",
    "print(f\"\\nüéØ Dataset is ready for modeling with {len(df)} samples and {len(df.columns)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Summary and Next Steps\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "**Dataset Overview:**\n",
    "- Size: {len(df):,} rows √ó {len(df.columns)} columns\n",
    "- Numeric variables: {len(numeric_cols)}\n",
    "- Categorical variables: {len(categorical_cols)}\n",
    "- Missing data: {df.isnull().sum().sum()} total missing values\n",
    "\n",
    "**Data Quality:**\n",
    "- Duplicate rows: {df.duplicated().sum()}\n",
    "- Constant columns: {len([col for col in df.columns if df[col].nunique() <= 1])}\n",
    "- High cardinality columns: {len([col for col in categorical_cols if df[col].nunique() > 50])}\n",
    "\n",
    "**Key Relationships:**\n",
    "- [Add your domain-specific insights here]\n",
    "- [Summarize important correlations]\n",
    "- [Note any interesting patterns]\n",
    "\n",
    "### Preprocessing Recommendations\n",
    "\n",
    "1. **Missing Data:** [Strategy based on analysis]\n",
    "2. **Outliers:** [Treatment plan]\n",
    "3. **Feature Engineering:** [Potential new features]\n",
    "4. **Encoding:** [Categorical variable treatment]\n",
    "5. **Scaling:** [Normalization needs]\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Data Preprocessing:**\n",
    "   - Handle missing values\n",
    "   - Address outliers\n",
    "   - Encode categorical variables\n",
    "   - Scale numeric features\n",
    "\n",
    "2. **Feature Selection:**\n",
    "   - Remove highly correlated features\n",
    "   - Select features based on target correlation\n",
    "   - Consider dimensionality reduction\n",
    "\n",
    "3. **Modeling:**\n",
    "   - Split data for training/validation\n",
    "   - Choose appropriate algorithms\n",
    "   - Cross-validation strategy\n",
    "\n",
    "---\n",
    "*EDA completed using Data Analysis and Prediction Platform*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n",
   "name": "python3"
  },\n",
   "language_info": {\n",
   "codemirror_mode": {\n",
    "name": "ipython",\n",
    "version": 3\n",
   },\n",
   "file_extension": ".py",\n",
   "mimetype": "text/x-python",\n",
   "name": "python",\n",
   "nbconvert_exporter": "python",\n",
   "pygments_lexer": "ipython3",\n",
   "version": "3.8.0"\n",
  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}