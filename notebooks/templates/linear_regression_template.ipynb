{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Modeling Template\n",
    "\n",
    "Focused template for linear regression analysis using the Data Analysis and Prediction Platform.\n",
    "\n",
    "## Objectives\n",
    "- Build and evaluate linear regression models\n",
    "- Understand feature relationships\n",
    "- Validate model assumptions\n",
    "- Export model for production use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.stats as stats\n",
    "import requests\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "# Platform API configuration\n",
    "API_BASE_URL = \"http://localhost:8000\"\n",
    "print(\"🚀 Linear Regression Template Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset here\n",
    "# Replace with your actual data loading code\n",
    "\n",
    "# Example: Load via API\n",
    "# file_id = \"your-file-id\"\n",
    "# response = requests.get(f\"{API_BASE_URL}/data/{file_id}\")\n",
    "# df = pd.read_csv(response.json()['file_path'])\n",
    "\n",
    "# Example: Generate sample data\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "X_sample = np.random.randn(n, 3)\n",
    "y_sample = 2 * X_sample[:, 0] + 3 * X_sample[:, 1] - 1.5 * X_sample[:, 2] + np.random.randn(n) * 0.5\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'feature_1': X_sample[:, 0],\n",
    "    'feature_2': X_sample[:, 1], \n",
    "    'feature_3': X_sample[:, 2],\n",
    "    'target': y_sample\n",
    "})\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "target_col = 'target'  # Adjust as needed\n",
    "feature_cols = [col for col in df.columns if col != target_col]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[target_col]\n",
    "\n",
    "print(f\"Features: {feature_cols}\")\n",
    "print(f\"Target: {target_col}\")\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for multicollinearity\n",
    "correlation_matrix = X.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Highlight high correlations\n",
    "high_corr = np.where(np.abs(correlation_matrix) > 0.8)\n",
    "high_corr_pairs = [(correlation_matrix.index[x], correlation_matrix.columns[y], correlation_matrix.iloc[x, y]) \n",
    "                   for x, y in zip(*high_corr) if x != y and x < y]\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(\"⚠️ High correlations detected:\")\n",
    "    for feat1, feat2, corr in high_corr_pairs:\n",
    "        print(f\"  {feat1} - {feat2}: {corr:.3f}\")\n",
    "else:\n",
    "    print(\"✅ No concerning multicollinearity detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=None\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Optional: Scale features (useful for regularized models)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"✅ Data scaled and ready for modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple regression models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=0.1)\n",
    "}\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    if 'Ridge' in name or 'Lasso' in name:\n",
    "        # Use scaled data for regularized models\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred_train = model.predict(X_train_scaled)\n",
    "        y_pred_test = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        # Use original data for linear regression\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    \n",
    "    model_results[name] = {\n",
    "        'model': model,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'train_mse': train_mse,\n",
    "        'test_mse': test_mse,\n",
    "        'test_mae': test_mae,\n",
    "        'y_pred_test': y_pred_test\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Train R²: {train_r2:.4f}\")\n",
    "    print(f\"  Test R²: {test_r2:.4f}\")\n",
    "    print(f\"  Test MSE: {test_mse:.4f}\")\n",
    "    print(f\"  Test MAE: {test_mae:.4f}\")\n",
    "\n",
    "print(\"\\n✅ All models trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(model_results.keys()),\n",
    "    'Train_R2': [results['train_r2'] for results in model_results.values()],\n",
    "    'Test_R2': [results['test_r2'] for results in model_results.values()],\n",
    "    'Test_MSE': [results['test_mse'] for results in model_results.values()],\n",
    "    'Test_MAE': [results['test_mae'] for results in model_results.values()]\n",
    "})\n",
    "\n",
    "comparison_df['Overfitting'] = comparison_df['Train_R2'] - comparison_df['Test_R2']\n",
    "\n",
    "print(\"📊 Model Comparison:\")\n",
    "comparison_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on test R²\n",
    "best_model_name = comparison_df.loc[comparison_df['Test_R2'].idxmax(), 'Model']\n",
    "best_model = model_results[best_model_name]['model']\n",
    "best_predictions = model_results[best_model_name]['y_pred_test']\n",
    "\n",
    "print(f\"🏆 Best Model: {best_model_name}\")\n",
    "print(f\"Test R²: {model_results[best_model_name]['test_r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis\n",
    "residuals = y_test - best_predictions\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Predicted vs Actual\n",
    "axes[0, 0].scatter(y_test, best_predictions, alpha=0.6)\n",
    "axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0, 0].set_xlabel('Actual Values')\n",
    "axes[0, 0].set_ylabel('Predicted Values')\n",
    "axes[0, 0].set_title('Predicted vs Actual')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Residuals vs Predicted\n",
    "axes[0, 1].scatter(best_predictions, residuals, alpha=0.6)\n",
    "axes[0, 1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0, 1].set_xlabel('Predicted Values')\n",
    "axes[0, 1].set_ylabel('Residuals')\n",
    "axes[0, 1].set_title('Residuals vs Predicted')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Histogram of residuals\n",
    "axes[1, 0].hist(residuals, bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Residuals')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Distribution of Residuals')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Q-Q plot for normality\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Q-Q Plot (Normality Check)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical tests\n",
    "_, p_value_normality = stats.shapiro(residuals)\n",
    "print(f\"\\n📊 Residual Analysis:\")\n",
    "print(f\"Mean residual: {residuals.mean():.6f}\")\n",
    "print(f\"Std residual: {residuals.std():.4f}\")\n",
    "print(f\"Normality test p-value: {p_value_normality:.4f}\")\n",
    "\n",
    "if p_value_normality > 0.05:\n",
    "    print(\"✅ Residuals appear normally distributed\")\n",
    "else:\n",
    "    print(\"⚠️ Residuals may not be normally distributed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (for linear regression)\n",
    "if best_model_name == 'Linear Regression':\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Coefficient': best_model.coef_,\n",
    "        'Abs_Coefficient': np.abs(best_model.coef_)\n",
    "    }).sort_values('Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = ['red' if coef < 0 else 'blue' for coef in feature_importance['Coefficient']]\n",
    "    bars = plt.barh(feature_importance['Feature'], feature_importance['Coefficient'], color=colors, alpha=0.7)\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.title('Feature Coefficients (Red=Negative, Blue=Positive)')\n",
    "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"📊 Feature Coefficients:\")\n",
    "    print(feature_importance)\n",
    "    print(f\"\\nIntercept: {best_model.intercept_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "cv_scores = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    if 'Ridge' in name or 'Lasso' in name:\n",
    "        scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "    else:\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "    \n",
    "    cv_scores[name] = scores\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  CV R² scores: {scores}\")\n",
    "    print(f\"  Mean CV R²: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
    "    print()\n",
    "\n",
    "# Plot CV scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "cv_data = [cv_scores[name] for name in cv_scores.keys()]\n",
    "plt.boxplot(cv_data, labels=cv_scores.keys())\n",
    "plt.ylabel('R² Score')\n",
    "plt.title('Cross-Validation Performance Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Export and API Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_filename = f\"linear_regression_model_{timestamp}.joblib\"\n",
    "model_path = Path(\"../data/processed/models\") / model_filename\n",
    "model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model and scaler\n",
    "model_package = {\n",
    "    'model': best_model,\n",
    "    'scaler': scaler if 'Ridge' in best_model_name or 'Lasso' in best_model_name else None,\n",
    "    'feature_columns': feature_cols,\n",
    "    'model_name': best_model_name,\n",
    "    'metrics': model_results[best_model_name]\n",
    "}\n",
    "\n",
    "joblib.dump(model_package, model_path)\n",
    "print(f\"✅ Model saved to: {model_path}\")\n",
    "\n",
    "# Create model metadata\n",
    "model_metadata = {\n",
    "    'model_id': str(uuid.uuid4()),\n",
    "    'model_type': 'linear_regression',\n",
    "    'model_name': best_model_name,\n",
    "    'file_path': str(model_path),\n",
    "    'created_at': datetime.now().isoformat(),\n",
    "    'features': feature_cols,\n",
    "    'target': target_col,\n",
    "    'performance': {\n",
    "        'test_r2': float(model_results[best_model_name]['test_r2']),\n",
    "        'test_mse': float(model_results[best_model_name]['test_mse']),\n",
    "        'test_mae': float(model_results[best_model_name]['test_mae'])\n",
    "    },\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test)\n",
    "}\n",
    "\n",
    "print(\"📊 Model Metadata:\")\n",
    "print(json.dumps(model_metadata, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Register model with platform API (uncomment when API is available)\n",
    "# try:\n",
    "#     response = requests.post(\n",
    "#         f\"{API_BASE_URL}/models/register\",\n",
    "#         json=model_metadata,\n",
    "#         headers={\"Content-Type\": \"application/json\"}\n",
    "#     )\n",
    "#     if response.status_code == 200:\n",
    "#         print(f\"✅ Model registered with API: {response.json()}\")\n",
    "#     else:\n",
    "#         print(f\"❌ Failed to register model: {response.status_code}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"⚠️ Could not connect to API: {e}\")\n",
    "\n",
    "print(\"📝 Model ready for production deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(new_data, model_package):\n",
    "    \"\"\"Make predictions using the trained model.\"\"\"\n",
    "    model = model_package['model']\n",
    "    scaler = model_package['scaler']\n",
    "    feature_cols = model_package['feature_columns']\n",
    "    \n",
    "    # Ensure correct feature order\n",
    "    if isinstance(new_data, dict):\n",
    "        new_data = pd.DataFrame([new_data])\n",
    "    \n",
    "    new_data = new_data[feature_cols]\n",
    "    \n",
    "    # Scale if necessary\n",
    "    if scaler is not None:\n",
    "        new_data_scaled = scaler.transform(new_data)\n",
    "        prediction = model.predict(new_data_scaled)\n",
    "    else:\n",
    "        prediction = model.predict(new_data)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Example prediction\n",
    "sample_input = {col: X_test.iloc[0][col] for col in feature_cols}\n",
    "prediction = make_prediction(sample_input, model_package)\n",
    "\n",
    "print(f\"📊 Sample Prediction:\")\n",
    "print(f\"Input: {sample_input}\")\n",
    "print(f\"Predicted: {prediction[0]:.4f}\")\n",
    "print(f\"Actual: {y_test.iloc[0]:.4f}\")\n",
    "print(f\"Error: {abs(prediction[0] - y_test.iloc[0]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Model Performance\n",
    "- **Best Model**: {best_model_name}\n",
    "- **Test R²**: {model_results[best_model_name]['test_r2']:.4f}\n",
    "- **Test MSE**: {model_results[best_model_name]['test_mse']:.4f}\n",
    "- **Test MAE**: {model_results[best_model_name]['test_mae']:.4f}\n",
    "\n",
    "### Key Findings\n",
    "- Dataset size: {len(df)} samples, {len(feature_cols)} features\n",
    "- Model explains {model_results[best_model_name]['test_r2']*100:.1f}% of variance in test data\n",
    "- Residuals analysis: [Add your interpretation]\n",
    "\n",
    "### Next Steps\n",
    "1. Deploy model using platform API\n",
    "2. Monitor model performance in production\n",
    "3. Consider feature engineering for improvement\n",
    "4. Collect new data for model retraining\n",
    "\n",
    "---\n",
    "*Linear Regression Analysis completed using DAPP*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}